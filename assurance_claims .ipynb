{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1b1VikNEUFbMc8A-3Km9nQNyOxPafedmo","authorship_tag":"ABX9TyOY/AV1ZPb2Y/x97Ir7fHdn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Install PySpark in Colab\n","!pip install pyspark\n","!pip install findspark\n","!pip install delta-spark\n","!pip install pyspark\n","!apt-get update -y\n","!apt-get install openjdk-17-jdk -y\n","# Set Java path for Colab\n","import os\n","#os.environ['JAVA_HOME'] = r'C:\\Program Files\\Eclipse Adoptium\\jdk-21.0.8.9-hotspot'\n","\n","#'/usr/lib/jvm/java-11-openjdk-amd64'\n","\n","# Définir JAVA_HOME pour OpenJDK 17\n","os.environ['JAVA_HOME'] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n","\n","# Ajouter le bin de Java au PATH\n","os.environ['PATH'] = os.environ['JAVA_HOME'] + \"/bin:\" + os.environ['PATH']\n","\n","# Vérifier que JAVA_HOME est bien défini\n","print(\"JAVA_HOME:\", os.environ['JAVA_HOME'])\n","\n","\n","\n","'''\n","delta_spark_jar = \"/content/drive/MyDrive/data/delta-spark_2.12-3.2.1.jar\"\n","C:\\Program Files\\Eclipse Adoptium\\jdk-21.0.8.9-hotspot\\bin\\java.exe\n","C:\\Program Files\\Common Files\\Oracle\\Java\\javapath\\java.exe\n","C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\java8path\\java.exe\n","'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":680},"id":"Bb4yxygcmwCH","outputId":"815a944d-40b3-4167-fffb-f28bf2f205dc","executionInfo":{"status":"ok","timestamp":1758570361577,"user_tz":240,"elapsed":40102,"user":{"displayName":"Abdoulaye Issa","userId":"11578120158179831578"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<>:27: SyntaxWarning: invalid escape sequence '\\P'\n","<>:27: SyntaxWarning: invalid escape sequence '\\P'\n","/tmp/ipython-input-2416498830.py:27: SyntaxWarning: invalid escape sequence '\\P'\n","  C:\\Program Files\\Eclipse Adoptium\\jdk-21.0.8.9-hotspot\\bin\\java.exe\n"]},{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (4.0.1)\n","Requirement already satisfied: py4j==0.10.9.9 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.9)\n","Requirement already satisfied: findspark in /usr/local/lib/python3.12/dist-packages (2.0.1)\n","Requirement already satisfied: delta-spark in /usr/local/lib/python3.12/dist-packages (4.0.0)\n","Requirement already satisfied: pyspark>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from delta-spark) (4.0.1)\n","Requirement already satisfied: importlib-metadata>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from delta-spark) (8.7.0)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata>=1.0.0->delta-spark) (3.23.0)\n","Requirement already satisfied: py4j==0.10.9.9 in /usr/local/lib/python3.12/dist-packages (from pyspark>=4.0.0->delta-spark) (0.10.9.9)\n","Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (4.0.1)\n","Requirement already satisfied: py4j==0.10.9.9 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.9)\n","Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n","Hit:2 http://security.ubuntu.com/ubuntu jammy-security InRelease\n","Get:3 https://cli.github.com/packages stable InRelease [3,917 B]\n","Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n","Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n","Hit:6 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n","Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n","Hit:8 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n","Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n","Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n","Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n","Fetched 3,917 B in 1s (2,916 B/s)\n","Reading package lists... Done\n","W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","openjdk-17-jdk is already the newest version (17.0.16+8~us1-0ubuntu1~22.04.1).\n","0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n","JAVA_HOME: /usr/lib/jvm/java-17-openjdk-amd64\n"]},{"output_type":"execute_result","data":{"text/plain":["'\\ndelta_spark_jar = \"/content/drive/MyDrive/data/delta-spark_2.12-3.2.1.jar\"\\nC:\\\\Program Files\\\\Eclipse Adoptium\\\\jdk-21.0.8.9-hotspot\\x08in\\\\java.exe\\nC:\\\\Program Files\\\\Common Files\\\\Oracle\\\\Java\\\\javapath\\\\java.exe\\nC:\\\\Program Files (x86)\\\\Common Files\\\\Oracle\\\\Java\\\\java8path\\\\java.exe\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":1}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nkDVDXgEFblv"},"outputs":[],"source":["from pyspark.sql import SparkSession\n","\n","\n","#import os\n","#os.environ['JAVA_HOME'] = r'C:\\Program Files\\Eclipse Adoptium\\jdk-21.0.8.9-hotspot'\n","\n","# Create a new SparkSession\n","spark = (SparkSession\n","         .builder\n","         .appName(\"read-csv-data\")\n","         .master(\"local[*]\")  # Use local mode\n","         .config(\"spark.rpc.message.maxSize\", \"2047\")\n","         .config(\"spark.network.maxFrameSize\", \"2047m\")\n","         .config(\"spark.serializer.objectStreamReset\", \"10000\")\n","         .config(\"spark.rpc.askTimeout\", \"120s\")\n","         .config(\"spark.network.timeout\", \"240s\")\n","         .config(\"spark.executor.memory\", \"512m\")\n","         .config(\"spark.driver.memory\", \"4g\") # Increased driver memory again\n","         #.config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.1.0\")#delta-core_2.12-2.4.0.jar\n","         #\n","         .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n","         .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n","         #\n","         .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0\")\n","         .config(\"spark.sql.adaptive.enabled\", \"false\")\n","         .config(\"spark.dynamicAllocation.enabled\", \"false\")\n","         .config(\"spark.sql.shuffle.partitions\", \"8\")\n","         .getOrCreate())\n","\n","# Set log level to ERROR\n","spark.sparkContext.setLogLevel(\"ERROR\")"]},{"cell_type":"code","source":["# Read CSV file into a DataFrame\n","df = (spark.read\n","      .format(\"csv\")\n","      .option(\"header\", \"true\")\n","      .load(\"../data/insurance_claims.csv\"))\n","\n","# Alternatively\n","## If your CSV file does not have a header row\n","\n","#df = (spark.read\n","#      .format(\"csv\")\n","#      .option(\"header\", \"false\") # When the CSV file does not have any headers\n","#      .load(\"../data/netflix_titles.csv\"))\n","\n","df.show(10, truncate=30)\n","\n"],"metadata":{"id":"Dwcf8dqdOLJg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n","\n","# Define a Schema\n","schema = StructType([\n","\n","    StructField(\"months_as_customer\", StringType(), True),\n","    StructField(\"age\", StringType(), True),\n","    StructField(\"policy_number\", StringType(), True),\n","    StructField(\"policy_bind_date\", StringType(), True),\n","    StructField(\"policy_state\", StringType(), True),\n","    StructField(\"policy_csl\", StringType(), True),\n","    #StructField(\"policy_deductable\", DateType(), True),\n","    StructField(\"policy_deductable\", StringType(), True),\n","    StructField(\"policy_annual_premium\", StringType(), True),\n","    StructField(\"umbrella_limit\", StringType(), True),\n","    StructField(\"insured_zip\", StringType(), True),\n","    StructField(\"insured_sex\", StringType(), True),\n","    StructField(\"insured_education_level\", StringType(), True),\n","    StructField(\"insured_occupation\", StringType(), True),\n","    StructField(\"insured_hobbies\", StringType(), True),\n","    StructField(\"insured_relationship\", StringType(), True),\n","    StructField(\"capital-gains\", StringType(), True),\n","    #StructField(\"capital-loss\", DateType(), True),\n","    StructField(\"capital-loss\", StringType(), True),\n","    #StructField(\"incident_date\", IntegerType(), True),\n","    StructField(\"incident_date\", StringType(), True),\n","    StructField(\"incident_type\", StringType(), True),\n","    StructField(\"collision_type\", StringType(), True),\n","    StructField(\"incident_severity\", StringType(), True),\n","    StructField(\"authorities_contacted\", StringType(), True),\n","    #StructField(\"incident_state\", DateType(), True),\n","    StructField(\"incident_state\", StringType(), True),\n","    #StructField(\"incident_city\", IntegerType(), True),\n","    StructField(\"incident_city\", StringType(), True),\n","    StructField(\"incident_location\", StringType(), True),\n","    StructField(\"incident_hour_of_the_day\", StringType(), True),\n","    StructField(\"number_of_vehicles_involved\", StringType(), True),\n","    StructField(\"property_damage\", StringType(), True),\n","    StructField(\"bodily_injuries\", StringType(), True),\n","    #StructField(\"witnesses\", DateType(), True),\n","    StructField(\"witnesses\", StringType(), True),\n","    #StructField(\"police_report_available\", IntegerType(), True),\n","    StructField(\"police_report_available\", StringType(), True),\n","    StructField(\"total_claim_amount\", StringType(), True),\n","    StructField(\"injury_claim\", StringType(), True),\n","    StructField(\"property_claim\", StringType(), True),\n","    StructField(\"vehicle_claim\", StringType(), True),\n","    StructField(\"auto_make\", StringType(), True),\n","    StructField(\"auto_model\", StringType(), True),\n","    StructField(\"auto_year\", StringType(), True),\n","    StructField(\"fraud_reported\", StringType(), True),\n","    StructField(\"_c39\", StringType(), True),\n","    ])\n","\n"],"metadata":{"id":"D4epbtCPOdKA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Read CSV file into a DataFrame\n","df = (spark.read.format(\"csv\")\n","      .option(\"header\", \"true\")\n","      .option(\"nullValue\", \"null\")\n","      .option(\"emptyValues\", \"\")\n","      .option(\"dateFormat\", \"LLLL d, y\")\n","      .schema(schema)\n","      .load(\"../data/insurance_claims.csv\"))\n","df.show(10, truncate=30)"],"metadata":{"id":"ViFHJsTQOd8w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql.functions import col, row_number, lead, lag, count, avg\n","from pyspark.sql.window import Window\n","\n","df_non_nul = df.filter(col('months_as_customer').isNotNull() & col('age').isNotNull() &\n","               col('policy_state').isNotNull() & col('policy_annual_premium').isNotNull() &\n","               col('insured_zip').isNotNull() & col('insured_sex').isNotNull() &\n","               col('insured_education_level').isNotNull() & col('insured_occupation').isNotNull() &\n","               col('insured_hobbies').isNotNull() & col('insured_relationship').isNotNull() &\n","               col('incident_date').isNotNull() & col('incident_type').isNotNull() &\n","               col('collision_type').isNotNull() & col('collision_type').isNotNull() &\n","               col('incident_severity').isNotNull() & col('authorities_contacted').isNotNull() &\n","               col('incident_state').isNotNull() & col('incident_city').isNotNull() &\n","               col('incident_location').isNotNull() & col('incident_hour_of_the_day').isNotNull() &\n","               col('number_of_vehicles_involved').isNotNull() & col('police_report_available').isNotNull() &\n","               col('total_claim_amount').isNotNull() & col('injury_claim').isNotNull() &\n","               col('property_claim').isNotNull() & col('vehicle_claim').isNotNull() &\n","               col('auto_make').isNotNull() & col('auto_model').isNotNull() &\n","               col('auto_year').isNotNull())\n","\n","\n","window_spec = Window.partitionBy(\"insured_sex\").orderBy(\"age\")\n","result = df_non_nul.withColumn(\"row_number\", row_number().over(window_spec))\n","result.select(\n","               \"age\"\n","               ,\"insured_sex\"\n","               ,\"row_number\"\n","    ).show()"],"metadata":{"id":"iDVNz6gnOmk4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Nested Window Functions\n","\n","window_spec = Window.partitionBy(\"insured_sex\").orderBy(\"age\")\n","df_count_sex = result.withColumn(\"count_sex\", count(\"insured_sex\").over(window_spec))\n","df_count_sex.show()"],"metadata":{"id":"vMTsUGTyOmbx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["grouped_df_age = df.groupBy(\"age\")\n","\n","# Count the number of rows in each group\n","count_df_age = grouped_df_age.count()\n","count_df_age.show()"],"metadata":{"id":"2zJrBoEaPejB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql.functions import max, col\n","'''\n","# Apply custom aggregation using max\n","max_age_df = count_df_age.agg(max(col(\"count\")))\n","max_age_df.show()\n","'''\n","\n","grouped_df = df.groupBy(\"age\")\n","count_df = grouped_df.count()\n","count_df.show()\n","max_policy_annual_premium = grouped_df.agg(max(col(\"policy_annual_premium\").alias(\"max_premium\")))\n","max_policy_annual_premium.show()"],"metadata":{"id":"ak65u3Z2PhV5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["get_ipython().run_line_magic('load_ext', 'sparksql_magic')\n","get_ipython().run_line_magic('config', 'SparkSql.limit=20')"],"metadata":{"id":"GiuqkrdiPpmo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%sparksql\n","CREATE OR REPLACE TABLE default.insurance_claims_delta (\n","\n","\n","    months_as_customer STRING,\n","    age STRING,\n","    policy_number STRING,\n","    policy_bind_date STRING,\n","    policy_state STRING,\n","    policy_csl STRING,\n","    policy_deductable STRING,\n","    policy_annual_premium STRING,\n","    umbrella_limit STRING,\n","    insured_zip STRING,\n","    insured_sex STRING,\n","    insured_education_level STRING,\n","    insured_occupation STRING,\n","    insured_hobbies STRING,\n","    insured_relationship STRING,\n","    capital_gains STRING,\n","    capital_loss STRING,\n","    incident_date STRING,\n","    incident_type STRING,\n","    collision_type STRING,\n","    incident_severity STRING,\n","    authorities_contacted STRING,\n","    incident_state STRING,\n","    incident_city STRING,\n","    incident_location STRING,\n","    incident_hour_of_the_day STRING,\n","    number_of_vehicles_involved STRING,\n","    property_damage STRING,\n","    bodily_injuries STRING,\n","    witnesses STRING,\n","    police_report_available STRING,\n","    total_claim_amount STRING,\n","    injury_claim STRING,\n","    property_claim STRING,\n","    vehicle_claim STRING,\n","    auto_make STRING,\n","    auto_model STRING,\n","    auto_year STRING,\n","    fraud_reported STRING,\n","    _c39 STRING\n","\n","\n",") USING DELTA LOCATION 'insurance_claims/spark-warehouse/insurance_claimsv3';"],"metadata":{"id":"rEfY-YRVOmT6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = (spark.read\n","      .format(\"csv\")\n","      .option(\"header\", \"true\")\n","      .load(\"../data/insurance_claims.csv\"))"],"metadata":{"id":"Lvz9prC3OmQZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"default.insurance_claimsv4\")"],"metadata":{"id":"0_BnLTWcOmLL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%sparksql\n","SELECT * FROM default.insurance_claimsv4 LIMIT 3;"],"metadata":{"id":"KRGu6otKOl8Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = spark.read.format(\"delta\").load(\"/opt/workspace/insurance_claims/spark-warehouse/insurance_claimsv4/\")"],"metadata":{"id":"pnrnUkXVQR0A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.printSchema()"],"metadata":{"id":"0T0iQnTtQRxI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%sparksql\n","SELECT * FROM delta.`/opt/workspace/insurance_claims/spark-warehouse/insurance_claimsv4/` LIMIT 3;\n"],"metadata":{"id":"PcIP0HKMQRuJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# For PySpark:\n","deltaTable = DeltaTable.forPath(spark, \"/opt/workspace/insurance_claims/spark-warehouse/insurance_claimsv4\")"],"metadata":{"id":"gUwkoD1_Qbwa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["deltaTable.toDF().show(5)"],"metadata":{"id":"QfJS5DszQRnY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql.functions import expr, lit\n","# Update director to not have nulls\n","deltaTable.update(\n","  condition = expr(\"policy_number IS NULL\"),\n","  set = { \"policy_number\": lit(\"\") })"],"metadata":{"id":"aH8r6_G6QkOR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%sparksql\n","UPDATE delta.`/opt/workspace/insurance_claims/spark-warehouse/insurance_claimsv4` SET policy_state = \"\" WHERE policy_state IS NULL;"],"metadata":{"id":"Q7rs8D7wQkLS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%sparksql\n","CREATE OR REPLACE TABLE default.insurance_claimsv3_cdf (\n","    --ici je cree une table bronze\n","    months_as_customer STRING,\n","    age STRING,\n","    policy_number STRING,\n","    policy_bind_date STRING,\n","    policy_state STRING,\n","    policy_csl STRING,\n","    policy_deductable STRING,\n","    policy_annual_premium STRING,\n","    umbrella_limit STRING,\n","    insured_zip STRING,\n","    insured_sex STRING,\n","    insured_education_level STRING,\n","    insured_occupation STRING,\n","    insured_hobbies STRING,\n","    insured_relationship STRING,\n","    capital_gains STRING,\n","    capital_loss STRING,\n","    incident_date STRING,\n","    incident_type STRING,\n","    collision_type STRING,\n","    incident_severity STRING,\n","    authorities_contacted STRING,\n","    incident_state STRING,\n","    incident_city STRING,\n","    incident_location STRING,\n","    incident_hour_of_the_day STRING,\n","    number_of_vehicles_involved STRING,\n","    property_damage STRING,\n","    bodily_injuries STRING,\n","    witnesses STRING,\n","    police_report_available STRING,\n","    total_claim_amount STRING,\n","    injury_claim STRING,\n","    property_claim STRING,\n","    vehicle_claim STRING,\n","    auto_make STRING,\n","    auto_model STRING,\n","    auto_year STRING,\n","    fraud_reported STRING,\n","    _c39 STRING\n","\n","\n",") USING DELTA LOCATION 'insurance_claims/spark-warehouse/insurance_claimsv3_cdf'\n","TBLPROPERTIES (delta.enableChangeDataFeed = true, medallionLevel = 'bronze');"],"metadata":{"id":"MtJilE_7QkIB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Read CSV file into a DataFrame\n","# Initial Load of Bronze Table\n","df = (spark.read\n","      .format(\"csv\")\n","      .option(\"header\", \"true\")\n","      .load(\"../data/insurance_claims.csv\"));\n","#Manually Align Columns (je me suis rendu compte )\n","df = df.withColumnRenamed(\"capital-gains\", \"capital_gains\") \\\n","       .withColumnRenamed(\"capital-loss\", \"capital_loss\")\n","\n","df.write.format(\"delta\").mode(\"append\").saveAsTable(\"default.insurance_claimsv3_cdf\")\n","df.show(5)"],"metadata":{"id":"qhVn08tnQkDR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%sparksql\n","CREATE OR REPLACE TABLE default.insurance_claimsv3_cleansed (\n","    ---ici je cree un table silver en utilisant le medallion architecture\n","    months_as_customer STRING,\n","    age STRING,\n","    policy_number STRING,\n","    policy_bind_date STRING,\n","    policy_state STRING,\n","    policy_csl STRING,\n","    policy_deductable STRING,\n","    policy_annual_premium STRING,\n","    umbrella_limit STRING,\n","    insured_zip STRING,\n","    insured_sex STRING,\n","    insured_education_level STRING,\n","    insured_occupation STRING,\n","    insured_hobbies STRING,\n","    insured_relationship STRING,\n","    capital_gains STRING,\n","    capital_loss STRING,\n","    incident_date STRING,\n","    incident_type STRING,\n","    collision_type STRING,\n","    incident_severity STRING,\n","    authorities_contacted STRING,\n","    incident_state STRING,\n","    incident_city STRING,\n","    incident_location STRING,\n","    incident_hour_of_the_day STRING,\n","    number_of_vehicles_involved STRING,\n","    property_damage STRING,\n","    bodily_injuries STRING,\n","    witnesses STRING,\n","    police_report_available STRING,\n","    total_claim_amount STRING,\n","    injury_claim STRING,\n","    property_claim STRING,\n","    vehicle_claim STRING,\n","    auto_make STRING,\n","    auto_model STRING,\n","    auto_year STRING,\n","    fraud_reported STRING,\n","    _c39 STRING\n","\n",") USING DELTA LOCATION '/opt/workspace/data/delta_lake/insurance_claimsv3_cleansed'\n","TBLPROPERTIES (delta.enableChangeDataFeed = true, medallionLevel = 'silver'\n",", updatedFromTable= 'default.insurance_claimsv3_cdf', updatedFromTableVersion= '2');"],"metadata":{"id":"UV2swj9QQRh4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#get the value of the last Updated Version from the silver table\n","lastUpdateVersion = int(spark.sql(\"SHOW TBLPROPERTIES default.insurance_claimsv3_cleansed ('updatedFromTableVersion')\").first()[\"value\"])+1\n","lastUpdateVersion"],"metadata":{"id":"EIBt9ul1RDSZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#get the value of the last Updated Version from the silver table\n","latestVersion = spark.sql(\"DESCRIBE HISTORY default.insurance_claimsv3_cdf\").first()[\"version\"]\n","latestVersion"],"metadata":{"id":"20IDmb0nQ_J4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#%%sparksql\n","#La fonction table_changes est  utilisée pour obtenir les changements dans une table Delta.\n","query = f\"\"\"\n","CREATE OR REPLACE TEMPORARY VIEW bronzeTable_latest_version as\n","SELECT * FROM (\n","    SELECT *,\n","        RANK() OVER (\n","        PARTITION BY (lower(months_as_customer), lower(age), lower(policy_number), policy_bind_date)\n","        ORDER BY policy_annual_premium DESC) as prix_de_la_police\n","    FROM table_changes('default.insurance_claimsv3_cdf',{lastUpdateVersion},{latestVersion})\n","    WHERE policy_annual_premium IS NOT NULL AND age IS NOT NULL AND policy_number IS NOT NULL AND  _change_type !='update_preimage'\n",")\n","WHERE policy_deductable = 1;\n","\"\"\"\n","\n","df = spark.sql(query)\n","df.show()"],"metadata":{"id":"46Yd6zhXQ_HR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%sparksql\n","ALTER TABLE default.insurance_claimsv3_cleansed SET TBLPROPERTIES(updatedFromTableVersion = {latestVersion});"],"metadata":{"id":"Yeym_Eg4Q_C5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%sparksql\n","--Update Bronze Table\n","-- Mise à jour des valeurs dans la table\n","UPDATE default.insurance_claimsv3_cdf\n","SET\n","    -- Mettre à 0 si l'âge est inférieur à 16 ou supérieur à 100\n","    age = CASE\n","            WHEN age < 16 OR age > 100 THEN 0\n","            ELSE age\n","          END,\n","\n","    -- Mettre à 0 si le montant total des réclamations est négatif ou si le mois d'abonnement est négatif\n","    total_claim_amount = CASE\n","                           WHEN total_claim_amount < 0 OR months_as_customer < 0 THEN 0\n","                           ELSE total_claim_amount\n","                         END,\n","\n","    -- Mettre à 0 si le numéro de police est NULL\n","    policy_number = CASE\n","                      WHEN policy_number IS NULL THEN 0\n","                      ELSE policy_number\n","                    END,\n","\n","    -- Mettre à 0 si la date de l'incident est NULL\n","    incident_date = CASE\n","                      WHEN incident_date IS NULL THEN 0\n","                      ELSE incident_date\n","                    END\n",";"],"metadata":{"id":"NVpdav-CRO_T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results = spark.sql(\" SELECT * FROM default.insurance_claimsv3_cdf LIMIT 3\")\n","results.show()"],"metadata":{"id":"gwg_CK0JRO7q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%sparksql\n","-- Ajouter la colonne missing_data si elle n'existe pas\n","ALTER TABLE default.insurance_claimsv3_cdf ADD COLUMNS (missing_data BOOLEAN);"],"metadata":{"id":"Nr3VsaCxRO27"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%sparksql\n","-- Mise à jour de la table pour marquer les données manquantes\n","UPDATE default.insurance_claimsv3_cdf\n","SET\n","    missing_data = CASE\n","                     WHEN policy_number IS NULL OR policy_number = '' THEN TRUE\n","                     WHEN incident_date IS NULL OR incident_date = '' THEN TRUE\n","                     WHEN total_claim_amount IS NULL OR total_claim_amount = '' THEN TRUE\n","                     ELSE FALSE\n","                   END;"],"metadata":{"id":"9khnMYkVROuR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%sparksql\n","-- Ajouter la colonne missing_data si elle n'existe pas\n","ALTER TABLE default.insurance_claimsv3_cdf ADD COLUMNS (age_group STRING);"],"metadata":{"id":"TbyuAON-Q-6Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%sparksql\n","-- Ajouter la colonne missing_data si elle n'existe pas\n","ALTER TABLE default.insurance_claimsv3_cdf ADD COLUMNS (fraud_reported_bool BOOLEAN);"],"metadata":{"id":"ww7fyqs9ej_L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZlFUxrP3ejtk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%sparksql\n","-- Mise à jour de la table pour marquer les groupes d'âge\n","UPDATE default.insurance_claimsv3_cdf\n","SET\n","    age_group = CASE\n","                  WHEN age < 25 THEN 'Young'\n","                  WHEN age < 35 THEN 'Young_Adult'\n","                  WHEN age < 50 THEN 'Middle_Age'\n","                  WHEN age < 65 THEN 'Senior'\n","                  ELSE 'Elder'\n","                END;"],"metadata":{"id":"T3mjqLDJRjkC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[" results = spark.sql(\" SELECT * FROM default.insurance_claimsv3_cdf\")\n","#results = spark.sql(\" SELECT * FROM default.insurance_claimsv3_cdf LIMIT 3\")\n","results.show(5)"],"metadata":{"id":"KGvo807yXXG6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%sparksql\n","-- Mise à jour de la table pour marquer les groupes d'âge\n","UPDATE default.insurance_claimsv3_cdf\n","SET\n","    fraud_reported_bool = CASE\n","                  WHEN fraud_reported = \"Y\" THEN 'True'\n","                  ELSE 'False'\n","                END;"],"metadata":{"id":"BAE9LRmGRjgw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[" from pyspark.sql.functions import to_date, col\n","# Conversion des dates\n","results = results.withColumn(\"policy_bind_date_parsed\",\n","                        to_date(col(\"policy_bind_date\"), \"yyyy-MM-dd\")) \\\n","            .withColumn(\"incident_date_parsed\",\n","                        to_date(col(\"incident_date\"), \"yyyy-MM-dd\"))"],"metadata":{"id":"hMYnulJJRjdZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql.functions import datediff\n","# Création de variables dérivées\n","results = results.withColumn(\"days_policy_to_incident\",\n","                        datediff(col(\"incident_date_parsed\"),\n","                            col(\"policy_bind_date_parsed\"))) \\\n","                .withColumn(\"claim_to_premium_ratio\",\n","                            col(\"total_claim_amount\") / col(\"policy_annual_premium\"))"],"metadata":{"id":"_IhgTfwjifhc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql.functions import upper, trim, when, lit\n","# Standardisation des catégories\n","results = results.withColumn(\"insured_sex_clean\",\n","                        upper(trim(col(\"insured_sex\")))) \\\n","                .withColumn(\"fraud_reported_bool\",\n","                         when(upper(col(\"fraud_reported\")) == \"Y\", lit(True))\n","                            .otherwise(lit(False)))"],"metadata":{"id":"2fulhNWhigps"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Window specifications\n","state_window = Window.partitionBy(\"policy_state\")\n","occupation_window = Window.partitionBy(\"insured_occupation\")\n","customer_window = Window.partitionBy(\"insured_zip\").orderBy(\"incident_date_parsed\")"],"metadata":{"id":"GLP6fkN_fMhz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Agrégations par état\n","df_agg = results.withColumn(\"avg_claim_amount_by_state\",\n","                    avg(\"total_claim_amount\").over(state_window)) \\\n","            .withColumn(\"fraud_rate_by_state\",\n","                    avg(col(\"fraud_reported_bool\").cast(\"int\")).over(state_window))"],"metadata":{"id":"2dO-ycYlfMWy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Agrégations par profession\n","#df_agg = df_agg.withColumn(\"avg_premium_by_occupation\",\n","df_agg = results.withColumn(\"avg_premium_by_occupation\",\n","                    avg(\"policy_annual_premium\").over(occupation_window)) \\\n","                .withColumn(\"claim_count_by_occupation\",\n","                    count(\"*\").over(occupation_window))"],"metadata":{"id":"yEFGgcebfBSq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql.functions import sum, col, row_number\n","df_agg = df_agg.withColumn(\"total_claim_amount\", col(\"total_claim_amount\").cast(\"float\"))\n","# Tendances temporelles par zone\n","df_agg = df_agg.withColumn(\"running_total_claims_area\",\n","                              sum(\"total_claim_amount\").over(customer_window)) \\\n","                   .withColumn(\"incident_rank_in_area\",\n","                              row_number().over(customer_window))\n"],"metadata":{"id":"M-YZ76OYfTrD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results = spark.sql(\" SELECT * FROM default.insurance_claimsv3_cdf\")\n","#results = spark.sql(\" SELECT * FROM default.insurance_claimsv3_cdf LIMIT 3\")\n","results.show(5)"],"metadata":{"id":"2ctl8DL_fTfs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Fraud Agrégations par profession\n","df_agg = df.withColumn(\"fraud_rate_by_profession\",\n","                         avg(col(\"fraud_reported_bool\").cast(\"int\")).over(insured_occupation))"],"metadata":{"id":"o4w2UFXNRjTx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"JB392GQAix9T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ffI9khOgix4M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"tSjaRr5HixsN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Read CSV file into a DataFrame\n","df = (spark.read\n","      .format(\"csv\")\n","      .option(\"header\", \"true\")\n","      .load(\"/content/drive/MyDrive/data/insurance_claims.csv\"))\n","\n","# Alternatively\n","## If your CSV file does not have a header row\n","\n","#df = (spark.read\n","#      .format(\"csv\")\n","#      .option(\"header\", \"false\") # When the CSV file does not have any headers\n","#      .load(\"../data/netflix_titles.csv\"))"],"metadata":{"id":"YKg7GuZoFrxV","executionInfo":{"status":"error","timestamp":1758570386958,"user_tz":240,"elapsed":3601,"user":{"displayName":"Abdoulaye Issa","userId":"11578120158179831578"}},"colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"d44e5dd9-569b-4c86-bec1-2d6056737827"},"execution_count":null,"outputs":[{"output_type":"error","ename":"Py4JJavaError","evalue":"An error occurred while calling o58.load.\n: java.util.ServiceConfigurationError: org.apache.spark.sql.sources.DataSourceRegister: org.apache.spark.sql.delta.sources.DeltaDataSource Unable to get public no-arg constructor\n\tat java.base/java.util.ServiceLoader.fail(ServiceLoader.java:586)\n\tat java.base/java.util.ServiceLoader.getConstructor(ServiceLoader.java:679)\n\tat java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNextService(ServiceLoader.java:1240)\n\tat java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNext(ServiceLoader.java:1273)\n\tat java.base/java.util.ServiceLoader$2.hasNext(ServiceLoader.java:1309)\n\tat java.base/java.util.ServiceLoader$3.hasNext(ServiceLoader.java:1393)\n\tat scala.collection.convert.JavaCollectionWrappers$JIteratorWrapper.hasNext(JavaCollectionWrappers.scala:46)\n\tat scala.collection.StrictOptimizedIterableOps.filterImpl(StrictOptimizedIterableOps.scala:225)\n\tat scala.collection.StrictOptimizedIterableOps.filterImpl$(StrictOptimizedIterableOps.scala:222)\n\tat scala.collection.convert.JavaCollectionWrappers$JIterableWrapper.filterImpl(JavaCollectionWrappers.scala:83)\n\tat scala.collection.StrictOptimizedIterableOps.filter(StrictOptimizedIterableOps.scala:218)\n\tat scala.collection.StrictOptimizedIterableOps.filter$(StrictOptimizedIterableOps.scala:218)\n\tat scala.collection.convert.JavaCollectionWrappers$JIterableWrapper.filter(JavaCollectionWrappers.scala:83)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:661)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:740)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:58)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:100)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat java.base/java.util.ServiceLoader.fail(ServiceLoader.java:586)\n\t\tat java.base/java.util.ServiceLoader.getConstructor(ServiceLoader.java:679)\n\t\tat java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNextService(ServiceLoader.java:1240)\n\t\tat java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNext(ServiceLoader.java:1273)\n\t\tat java.base/java.util.ServiceLoader$2.hasNext(ServiceLoader.java:1309)\n\t\tat java.base/java.util.ServiceLoader$3.hasNext(ServiceLoader.java:1393)\n\t\tat scala.collection.convert.JavaCollectionWrappers$JIteratorWrapper.hasNext(JavaCollectionWrappers.scala:46)\n\t\tat scala.collection.StrictOptimizedIterableOps.filterImpl(StrictOptimizedIterableOps.scala:225)\n\t\tat scala.collection.StrictOptimizedIterableOps.filterImpl$(StrictOptimizedIterableOps.scala:222)\n\t\tat scala.collection.convert.JavaCollectionWrappers$JIterableWrapper.filterImpl(JavaCollectionWrappers.scala:83)\n\t\tat scala.collection.StrictOptimizedIterableOps.filter(StrictOptimizedIterableOps.scala:218)\n\t\tat scala.collection.StrictOptimizedIterableOps.filter$(StrictOptimizedIterableOps.scala:218)\n\t\tat scala.collection.convert.JavaCollectionWrappers$JIterableWrapper.filter(JavaCollectionWrappers.scala:83)\n\t\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:661)\n\t\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:740)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:58)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n\t\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\t\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\t\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n\t\tat scala.collection.immutable.List.foreach(List.scala:334)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)\n\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n\t\tat scala.util.Try$.apply(Try.scala:217)\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t\t... 20 more\nCaused by: java.lang.NoClassDefFoundError: scala/collection/GenTraversableOnce\n\tat java.base/java.lang.Class.getDeclaredConstructors0(Native Method)\n\tat java.base/java.lang.Class.privateGetDeclaredConstructors(Class.java:3373)\n\tat java.base/java.lang.Class.getConstructor0(Class.java:3578)\n\tat java.base/java.lang.Class.getConstructor(Class.java:2271)\n\tat java.base/java.util.ServiceLoader$1.run(ServiceLoader.java:666)\n\tat java.base/java.util.ServiceLoader$1.run(ServiceLoader.java:663)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:569)\n\tat java.base/java.util.ServiceLoader.getConstructor(ServiceLoader.java:674)\n\tat java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNextService(ServiceLoader.java:1240)\n\tat java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNext(ServiceLoader.java:1273)\n\tat java.base/java.util.ServiceLoader$2.hasNext(ServiceLoader.java:1309)\n\tat java.base/java.util.ServiceLoader$3.hasNext(ServiceLoader.java:1393)\n\tat scala.collection.convert.JavaCollectionWrappers$JIteratorWrapper.hasNext(JavaCollectionWrappers.scala:46)\n\tat scala.collection.StrictOptimizedIterableOps.filterImpl(StrictOptimizedIterableOps.scala:225)\n\tat scala.collection.StrictOptimizedIterableOps.filterImpl$(StrictOptimizedIterableOps.scala:222)\n\tat scala.collection.convert.JavaCollectionWrappers$JIterableWrapper.filterImpl(JavaCollectionWrappers.scala:83)\n\tat scala.collection.StrictOptimizedIterableOps.filter(StrictOptimizedIterableOps.scala:218)\n\tat scala.collection.StrictOptimizedIterableOps.filter$(StrictOptimizedIterableOps.scala:218)\n\tat scala.collection.convert.JavaCollectionWrappers$JIterableWrapper.filter(JavaCollectionWrappers.scala:83)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:661)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:740)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:58)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t... 20 more\nCaused by: java.lang.ClassNotFoundException: scala.collection.GenTraversableOnce\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\t... 90 more\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2855223282.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m       \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m       \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m       .load(\"/content/drive/MyDrive/data/insurance_claims.csv\"))\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Alternatively\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1362\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    328\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m                     format(target_id, \".\", name), value)\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o58.load.\n: java.util.ServiceConfigurationError: org.apache.spark.sql.sources.DataSourceRegister: org.apache.spark.sql.delta.sources.DeltaDataSource Unable to get public no-arg constructor\n\tat java.base/java.util.ServiceLoader.fail(ServiceLoader.java:586)\n\tat java.base/java.util.ServiceLoader.getConstructor(ServiceLoader.java:679)\n\tat java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNextService(ServiceLoader.java:1240)\n\tat java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNext(ServiceLoader.java:1273)\n\tat java.base/java.util.ServiceLoader$2.hasNext(ServiceLoader.java:1309)\n\tat java.base/java.util.ServiceLoader$3.hasNext(ServiceLoader.java:1393)\n\tat scala.collection.convert.JavaCollectionWrappers$JIteratorWrapper.hasNext(JavaCollectionWrappers.scala:46)\n\tat scala.collection.StrictOptimizedIterableOps.filterImpl(StrictOptimizedIterableOps.scala:225)\n\tat scala.collection.StrictOptimizedIterableOps.filterImpl$(StrictOptimizedIterableOps.scala:222)\n\tat scala.collection.convert.JavaCollectionWrappers$JIterableWrapper.filterImpl(JavaCollectionWrappers.scala:83)\n\tat scala.collection.StrictOptimizedIterableOps.filter(StrictOptimizedIterableOps.scala:218)\n\tat scala.collection.StrictOptimizedIterableOps.filter$(StrictOptimizedIterableOps.scala:218)\n\tat scala.collection.convert.JavaCollectionWrappers$JIterableWrapper.filter(JavaCollectionWrappers.scala:83)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:661)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:740)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:58)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:100)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat java.base/java.util.ServiceLoader.fail(ServiceLoader.java:586)\n\t\tat java.base/java.util.ServiceLoader.getConstructor(ServiceLoader.java:679)\n\t\tat java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNextService(ServiceLoader.java:1240)\n\t\tat java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNext(ServiceLoader.java:1273)\n\t\tat java.base/java.util.ServiceLoader$2.hasNext(ServiceLoader.java:1309)\n\t\tat java.base/java.util.ServiceLoader$3.hasNext(ServiceLoader.java:1393)\n\t\tat scala.collection.convert.JavaCollectionWrappers$JIteratorWrapper.hasNext(JavaCollectionWrappers.scala:46)\n\t\tat scala.collection.StrictOptimizedIterableOps.filterImpl(StrictOptimizedIterableOps.scala:225)\n\t\tat scala.collection.StrictOptimizedIterableOps.filterImpl$(StrictOptimizedIterableOps.scala:222)\n\t\tat scala.collection.convert.JavaCollectionWrappers$JIterableWrapper.filterImpl(JavaCollectionWrappers.scala:83)\n\t\tat scala.collection.StrictOptimizedIterableOps.filter(StrictOptimizedIterableOps.scala:218)\n\t\tat scala.collection.StrictOptimizedIterableOps.filter$(StrictOptimizedIterableOps.scala:218)\n\t\tat scala.collection.convert.JavaCollectionWrappers$JIterableWrapper.filter(JavaCollectionWrappers.scala:83)\n\t\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:661)\n\t\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:740)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:58)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n\t\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\t\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\t\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n\t\tat scala.collection.immutable.List.foreach(List.scala:334)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)\n\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n\t\tat scala.util.Try$.apply(Try.scala:217)\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t\t... 20 more\nCaused by: java.lang.NoClassDefFoundError: scala/collection/GenTraversableOnce\n\tat java.base/java.lang.Class.getDeclaredConstructors0(Native Method)\n\tat java.base/java.lang.Class.privateGetDeclaredConstructors(Class.java:3373)\n\tat java.base/java.lang.Class.getConstructor0(Class.java:3578)\n\tat java.base/java.lang.Class.getConstructor(Class.java:2271)\n\tat java.base/java.util.ServiceLoader$1.run(ServiceLoader.java:666)\n\tat java.base/java.util.ServiceLoader$1.run(ServiceLoader.java:663)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:569)\n\tat java.base/java.util.ServiceLoader.getConstructor(ServiceLoader.java:674)\n\tat java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNextService(ServiceLoader.java:1240)\n\tat java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNext(ServiceLoader.java:1273)\n\tat java.base/java.util.ServiceLoader$2.hasNext(ServiceLoader.java:1309)\n\tat java.base/java.util.ServiceLoader$3.hasNext(ServiceLoader.java:1393)\n\tat scala.collection.convert.JavaCollectionWrappers$JIteratorWrapper.hasNext(JavaCollectionWrappers.scala:46)\n\tat scala.collection.StrictOptimizedIterableOps.filterImpl(StrictOptimizedIterableOps.scala:225)\n\tat scala.collection.StrictOptimizedIterableOps.filterImpl$(StrictOptimizedIterableOps.scala:222)\n\tat scala.collection.convert.JavaCollectionWrappers$JIterableWrapper.filterImpl(JavaCollectionWrappers.scala:83)\n\tat scala.collection.StrictOptimizedIterableOps.filter(StrictOptimizedIterableOps.scala:218)\n\tat scala.collection.StrictOptimizedIterableOps.filter$(StrictOptimizedIterableOps.scala:218)\n\tat scala.collection.convert.JavaCollectionWrappers$JIterableWrapper.filter(JavaCollectionWrappers.scala:83)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:661)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:740)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:58)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t... 20 more\nCaused by: java.lang.ClassNotFoundException: scala.collection.GenTraversableOnce\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\t... 90 more\n"]}]},{"cell_type":"code","source":["df.show()"],"metadata":{"id":"p2yiV8t8G1Oc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Print schema of DataFrame\n","df.printSchema()"],"metadata":{"id":"TnILgmg_HAF1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n","\n","# Define a Schema\n","schema = StructType([\n","    StructField(\"months_as_customer\", StringType(), True),\n","    StructField(\"age\", StringType(), True),\n","    StructField(\"policy_number\", StringType(), True),\n","    StructField(\"policy_bind_date\", StringType(), True),\n","    StructField(\"policy_state\", StringType(), True),\n","    StructField(\"policy_csl\", StringType(), True),\n","    #StructField(\"policy_deductable\", DateType(), True),\n","    StructField(\"policy_deductable\", StringType(), True),\n","    StructField(\"policy_annual_premium\", StringType(), True),\n","    StructField(\"umbrella_limit\", StringType(), True),\n","    StructField(\"insured_zip\", StringType(), True),\n","    StructField(\"insured_sex\", StringType(), True),\n","    StructField(\"insured_education_level\", StringType(), True),\n","    StructField(\"insured_occupation\", StringType(), True),\n","    StructField(\"insured_hobbies\", StringType(), True),\n","    StructField(\"insured_relationship\", StringType(), True),\n","    StructField(\"capital-gains\", StringType(), True),\n","    #StructField(\"capital-loss\", DateType(), True),\n","    StructField(\"capital-loss\", StringType(), True),\n","    #StructField(\"incident_date\", IntegerType(), True),\n","    StructField(\"incident_date\", StringType(), True),\n","    StructField(\"incident_type\", StringType(), True),\n","    StructField(\"collision_type\", StringType(), True),\n","    StructField(\"incident_severity\", StringType(), True),\n","    StructField(\"authorities_contacted\", StringType(), True),\n","    #StructField(\"incident_state\", DateType(), True),\n","    StructField(\"incident_state\", StringType(), True),\n","    #StructField(\"incident_city\", IntegerType(), True),\n","    StructField(\"incident_city\", StringType(), True),\n","    StructField(\"incident_location\", StringType(), True),\n","    StructField(\"incident_hour_of_the_day\", StringType(), True),\n","    StructField(\"number_of_vehicles_involved\", StringType(), True),\n","    StructField(\"property_damage\", StringType(), True),\n","    StructField(\"bodily_injuries\", StringType(), True),\n","    #StructField(\"witnesses\", DateType(), True),\n","    StructField(\"witnesses\", StringType(), True),\n","    #StructField(\"police_report_available\", IntegerType(), True),\n","    StructField(\"police_report_available\", StringType(), True),\n","    StructField(\"total_claim_amount\", StringType(), True),\n","    StructField(\"injury_claim\", StringType(), True),\n","    StructField(\"property_claim\", StringType(), True),\n","    StructField(\"vehicle_claim\", StringType(), True),\n","    StructField(\"auto_make\", StringType(), True),\n","    StructField(\"auto_model\", StringType(), True),\n","    StructField(\"auto_year\", StringType(), True),\n","    StructField(\"fraud_reported\", StringType(), True),\n","    StructField(\"_c39\", StringType(), True),\n","    ])\n","\n","\n"],"metadata":{"id":"Dinh9GNQJd3S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Read CSV file into a DataFrame\n","df = (spark.read.format(\"csv\")\n","      .option(\"header\", \"true\")\n","      .option(\"nullValue\", \"null\")\n","      .option(\"emptyValues\", \"\")\n","      .option(\"dateFormat\", \"LLLL d, y\")\n","      .schema(schema)\n","      .load(\"/content/drive/MyDrive/data/insurance_claims.csv\"))"],"metadata":{"id":"kxT3Aw4wMsMH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.show()"],"metadata":{"id":"bnQdWRBBNIiF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_deduped = df.dropDuplicates([\n","               \"months_as_customer\"\n","               ,\"age\"\n","               ,\"policy_state\"\n","               ,\"policy_annual_premium\"\n","               ,\"insured_zip\"\n","               ,\"insured_sex\"\n","               ,\"insured_education_level\"\n","               ,\"insured_occupation\"\n","               ,\"insured_hobbies\"\n","               ,\"insured_relationship\"\n","               ,\"incident_date\"\n","               ,\"incident_type\"\n","               ,\"collision_type\"\n","               ,\"collision_type\"\n","               ,\"incident_severity\"\n","               ,\"authorities_contacted\"\n","               ,\"incident_state\"\n","               ,\"incident_city\"\n","               ,\"incident_location\"\n","               ,\"incident_hour_of_the_day\"\n","               ,\"number_of_vehicles_involved\"\n","               ,\"police_report_available\"\n","               ,\"total_claim_amount\"\n","               ,\"injury_claim\"\n","               ,\"property_claim\"\n","               ,\"vehicle_claim\"\n","               ,\"auto_make\"\n","               ,\"auto_model\"\n","               ,\"auto_year\"\n","    ])\n","\n","df_deduped.show()"],"metadata":{"id":"eUCEEEEQDwnU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Sort by year in ascending order\n","df_sorted = df.orderBy(\"incident_date\")\n","\n","df_sorted.show()\n","\n","# Sort by year in descending order, then by category in ascending order\n","df_sorted_descending = df.orderBy([\"incident_date\", \"age\"], ascending=[False, True])\n"],"metadata":{"id":"zY5SUEWwEqok"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql.functions import col\n","#filtered_df = df.filter(col(\"incident_date\").year > 2015)\n","#filtered_df.show()\n","\n","from pyspark.sql.functions import col, substring\n","\n","# Extraire les 4 premiers caractères (année)\n","df_year = df.withColumn(\"year\", substring(col(\"incident_date\"), 1, 4))\n","#df_day = df.withColumn(\"day\", substring(col(\"incident_date\"), 9, 2))\n","df_year.show()\n","\n"],"metadata":{"id":"Lqy-XCn8pMej"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["filtered_df = (\n","    df.filter(\n","        (col(\"auto_make\") == \"Nissan\")\n","        & (substring(col(\"incident_date\"), 1, 4) > 2010)))\n","\n","filtered_df.show()"],"metadata":{"id":"PHFJYn1RrtAO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["filtered_df = (\n","    df.filter(\n","        col(\"policy_state\")\n","        .isin([\"IN\", \"OH\",  \"IL\"])))\n","filtered_df.show(3)"],"metadata":{"id":"s7UJApensDXI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# filter the DataFrame based on a substring match\n","filtered_df = df.filter(col(\"incident_type\").like(\"%Vehicle%\"))\n","\n","# display the filtered DataFrame\n","filtered_df.show()"],"metadata":{"id":"3C-KJPjZsZi_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# filter the DataFrame based on a date range\n","filtered_df = df.filter((col(\"incident_date\") >= \"2010-09-05\") & (col(\"incident_date\") <= \"2020-09-01\"))\n","\n","# display the filtered DataFrame\n","filtered_df.show()"],"metadata":{"id":"Wrb0EaY4tDp5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["filtered_df = df.filter((col(\"incident_date\").between(\"2010-02-01\",\"2015-03-01\")))\n","\n","# display the filtered DataFrame\n","filtered_df.show()"],"metadata":{"id":"Tgs_76H3tvMQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Dropping rows with null values\n","df_dropna = df.dropna()\n","\n","# Displaying the DataFrame after dropping null values\n","df_dropna.show()"],"metadata":{"id":"4N2USWSWuLc3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Filling null values with a specific value\n","df_fillna = df.fillna(\"N/A\")\n","\n","# Displaying the DataFrame after filling null values\n","df_fillna.show()"],"metadata":{"id":"2fSwP_h-wohR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Filling null values with a specific value\n","df_fillna = df.fillna(\"N/A\")\n","\n","# Displaying the DataFrame after filling null values\n","df_fillna.show()"],"metadata":{"id":"DcDqxU6Jwslv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","df = spark.createDataFrame(df,\n","\n","  [\n","               \"months_as_customer\"\n","               ,\"age\"\n","               ,\"policy_state\"\n","               ,\"policy_annual_premium\"\n","               ,\"insured_zip\"\n","               ,\"insured_sex\"\n","               ,\"insured_education_level\"\n","               ,\"insured_occupation\"\n","               ,\"insured_hobbies\"\n","               ,\"insured_relationship\"\n","               ,\"incident_date\"\n","               ,\"incident_type\"\n","               ,\"collision_type\"\n","               ,\"collision_type\"\n","               ,\"incident_severity\"\n","               ,\"authorities_contacted\"\n","               ,\"incident_state\"\n","               ,\"incident_city\"\n","               ,\"incident_location\"\n","               ,\"incident_hour_of_the_day\"\n","               ,\"number_of_vehicles_involved\"\n","               ,\"police_report_available\"\n","               ,\"total_claim_amount\"\n","               ,\"injury_claim\"\n","               ,\"property_claim\"\n","               ,\"vehicle_claim\"\n","               ,\"auto_make\"\n","               ,\"auto_model\"\n","               ,\"auto_year\"\n","  ])\n","\n","  '''\n","# Ajouter l'import manquant\n","from pyspark.sql.functions import udf\n","from pyspark.sql.types import StringType\n","\n","# Define a UDF to handle null values\n","def process_insured_relationship(insured_relationship):\n","    if insured_relationship is None:\n","        return \"Unknown\"\n","    else:\n","        return insured_relationship.upper()\n","\n","# Register the UDF\n","process_insured_relationship_udf = udf(process_insured_relationship, StringType())\n","\n","# Apply the UDF to the DataFrame\n","df_with_processed_insured_relationship = df.withColumn(\"process_insured_relationship\", process_insured_relationship_udf(df[\"insured_relationship\"]))\n","\n","# Show the resulting DataFrame\n","df_with_processed_insured_relationship.show()"],"metadata":{"id":"NwHFeZ0Hx9L8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.ml.feature import Imputer\n","'''\n","#Pour utiliser ceci, il faut utiliser les colonnes numeriques\n","# Create a sample DataFrame with missing values\n","data = [\n","    (1, 2.0),\n","    (2, None),\n","    (3, 5.0),\n","    (4, None),\n","    (5, 7.0)\n","]\n","#df = spark.createDataFrame(data, [\"id\", \"value\"])\n","# collision_type  property_damage|\n","# Create an instance of Imputer and specify the input/output columns\n","imputer = Imputer(inputCols=[\"collision_type\"], outputCols=[\"imputed_collision_type\"])\n","\n","# Fit the imputer to the data and transform the DataFrame\n","imputer_model = imputer.fit(df)\n","imputed_df = imputer_model.transform(df)\n","\n","# Show the resulting DataFrame\n","imputed_df.show()\n","\n","'''"],"metadata":{"id":"khrWlAnB0qOo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#insured_sex insured_relationship\n","\n","def concat(insured_sex, insured_relationship):\n","    return insured_sex + \" \" + insured_relationship\n","\n","from pyspark.sql.functions import udf\n","concat_udf = udf(concat)\n","\n","from pyspark.sql.types import StringType\n","concat_udf = udf(concat, StringType())\n","\n","df = df.withColumn(\"insured_sex_with_insured_relationship\", concat_udf(df[\"insured_sex\"], df[\"insured_relationship\"]))\n","\n","df.show()\n","#on peut faire un regroupement selon insured_sex_with_insured_relationship = FEMALE own-child,  MALE unmarried, MALE husband, MALE wife\n","\n","\n"],"metadata":{"id":"DLRBcVDNAxN_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql.functions import col, row_number, lead, lag, count, avg\n","from pyspark.sql.window import Window\n","\n","df_non_nul = df.filter(col('months_as_customer').isNotNull() & col('age').isNotNull() &\n","               col('policy_state').isNotNull() & col('policy_annual_premium').isNotNull() &\n","               col('insured_zip').isNotNull() & col('insured_sex').isNotNull() &\n","               col('insured_education_level').isNotNull() & col('insured_occupation').isNotNull() &\n","               col('insured_hobbies').isNotNull() & col('insured_relationship').isNotNull() &\n","               col('incident_date').isNotNull() & col('incident_type').isNotNull() &\n","               col('collision_type').isNotNull() & col('collision_type').isNotNull() &\n","               col('incident_severity').isNotNull() & col('authorities_contacted').isNotNull() &\n","               col('incident_state').isNotNull() & col('incident_city').isNotNull() &\n","               col('incident_location').isNotNull() & col('incident_hour_of_the_day').isNotNull() &\n","               col('number_of_vehicles_involved').isNotNull() & col('police_report_available').isNotNull() &\n","               col('total_claim_amount').isNotNull() & col('injury_claim').isNotNull() &\n","               col('property_claim').isNotNull() & col('vehicle_claim').isNotNull() &\n","               col('auto_make').isNotNull() & col('auto_model').isNotNull() &\n","               col('auto_year').isNotNull())\n","\n","\n","window_spec = Window.partitionBy(\"insured_sex\").orderBy(\"age\")\n","result = df_non_nul.withColumn(\"row_number\", row_number().over(window_spec))\n","result.select(\n","               \"age\"\n","               ,\"insured_sex\"\n","               ,\"row_number\"\n","    ).show()\n"],"metadata":{"id":"Dk6H6xszG-Z3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Nested Window Functions\n","\n","window_spec = Window.partitionBy(\"insured_sex\").orderBy(\"age\")\n","#result = df_non_nul.withColumn(\"row_number\", row_number().over(window_spec))\n","\n","#result = df.withColumn(\"row_number\", row_number().over(window_spec))\n","#result.select(\"title\",\"country\",\"date_added\",\"row_number\").show()\n","\n","df_count_sex = result.withColumn(\"count_sex\", count(\"insured_sex\").over(window_spec))\n","\n","#window_spec = Window.partitionBy(\"country\").orderBy(\"release_year\")\n","#df = df.withColumn(\"running_total\", count(\"show_id\").over(window_spec))\n","df_count_sex.show()\n","\n","#df = df.withColumn(\"next_running_total\", lead(\"running_total\").over(window_spec))\n","#df = df.withColumn(\"diff\", df[\"next_running_total\"] - df[\"running_total\"])\n","\n","\n"],"metadata":{"id":"T-9MaJgbK3Io"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["grouped_df_age = df.groupBy(\"age\")\n","\n","# Count the number of rows in each group\n","count_df_age = grouped_df_age.count()\n","count_df_age.show()"],"metadata":{"id":"3l7gL_khO7ot"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql.functions import max, col\n","'''\n","# Apply custom aggregation using max\n","max_age_df = count_df_age.agg(max(col(\"count\")))\n","max_age_df.show()\n","'''\n","\n","grouped_df = df.groupBy(\"age\")\n","count_df = grouped_df.count()\n","count_df.show()\n","max_policy_annual_premium = grouped_df.agg(max(col(\"policy_annual_premium\").alias(\"max_premium\")))\n","max_policy_annual_premium.show()\n","\n","\n","\n","'''\n","# Correct - agg() retourne un DataFrame\n","max_policy_df = grouped_df.agg(max(col(\"policy_annual_premium\")).alias(\"max_premium\"))\n","max_policy_df.show()\n","\n","# Si vous voulez juste la valeur maximale\n","max_value = max_policy_df.collect()[0][\"max_premium\"]\n","print(f\"Maximum: {max_value}\")\n","\n","\n","\n","\n","grouped_df = df.groupBy(\"country\")\n","count_df = grouped_df.count()\n","count_df.show()\n","\n","# Apply custom aggregation using max\n","max_release_df = grouped_df.agg(max(col(\"date_added\")))\n","max_release_df.show()\n","'''"],"metadata":{"id":"TQvFS6vmPHrl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","'''\n","policy_state_df = (\n","    df.groupBy(\"policy_state\")\n","    .agg(\n","        count(\"policy_state\").alias(\"NumberOfstate\")\n","        ,max(\"policy_annual_premium \").alias(\"max_policy_annual_premium\")\n","        ,min('age').alias(\"minAge\")\n","        ,max('age').alias(\"maxAge\")\n","        ))\n","\n","policy_state_df.show(4)\n","\n","'''"],"metadata":{"id":"_9mRWWKeSSg4"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d7d13e99"},"source":["from delta import DeltaTable\n","\n","# Write the DataFrame as a Delta table\n","df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"default.insurance_claims\")\n","\n","# Verify by reading the Delta table\n","results = spark.sql(\" SELECT * FROM default.insurance_claims LIMIT 3\")\n","results.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9b0fc874"},"source":["### Subtask:\n","Create spark session with delta lake"]},{"cell_type":"markdown","metadata":{"id":"4cb9ee6b"},"source":["**Reasoning**:\n","Create a Spark session with Delta Lake support by explicitly adding the located Delta Lake JAR to the classpath configurations."]},{"cell_type":"code","metadata":{"id":"9ef2b69f"},"source":["from pyspark.sql import SparkSession\n","from delta import configure_spark_with_delta_pip, DeltaTable\n","import os\n","\n","# Stop any existing SparkSession to avoid conflicts\n","if 'spark' in locals() and isinstance(spark, SparkSession):\n","    spark.stop()\n","    print(\"Stopped existing SparkSession.\")\n","\n","# Define the path to the delta-core JAR provided by the user\n","delta_spark_jar = \"/content/drive/MyDrive/data/delta-spark_2.12-3.2.1.jar\"\n","print(f\"Using delta-core JAR: {delta_spark_jar}\")\n","\n","if os.path.exists(delta_spark_jar):\n","    # Create a new SparkSession with Delta Lake support by explicitly adding JAR to classpath\n","    builder = SparkSession.builder \\\n","        .appName(\"DeltaLakeExample\") \\\n","        .master(\"local[*]\") \\\n","        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n","        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n","        .config(\"spark.driver.extraClassPath\", delta_spark_jar) \\\n","        .config(\"spark.executor.extraClassPath\", delta_spark_jar) \\\n","        .config(\"spark.jars\", delta_spark_jar) # Also add to spark.jars for good measure\n","\n","\n","    spark = builder.getOrCreate()\n","\n","    # Set log level to ERROR\n","    spark.sparkContext.setLogLevel(\"ERROR\")\n","\n","    print(\"\\nSparkSession created successfully with Delta Lake support.\")\n","else:\n","    print(f\"Delta-core JAR not found at the specified path: {delta_spark_jar}. Please verify the path.\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Write the DataFrame as a Delta table\n","df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"default.insurance_claims\")\n","\n","# Verify by reading the Delta table\n","results = spark.sql(\" SELECT * FROM default.insurance_claims LIMIT 3\")\n","results.show()"],"metadata":{"id":"RYcLtuBouY9_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#%%sparksql\n","\n","#For PySpark:\n","deltaTable = DeltaTable.forPath(spark, \"/opt/workspace/insurance_claims/spark-warehouse/insurance_claimsv4\")\n","\n","# Convertir la DeltaTable en DataFrame car sinon on ne pourra pas utiliser la fonction select\n","df = deltaTable.toDF()  # Ici, on convertit la DeltaTable en DataFrame\n","\n","'''\n","df = deltaTable.select(\n","    col('value.age').alias('age'),\n","    col('incident_state.name').alias('incident_state'))\n","'''\n","\n","df_moyenne_age = (df.select('age', 'incident_state').groupBy('incident_state').agg(avg('age').alias('avg_age')))\n","\n","# Register the DeltaTable as a temporary view (on va enlever ceci car deltaTable do not have attribut createOrReplaceTempView car ce nest pas sql\n","#deltaTable.createOrReplaceTempView(\"delta_view\")\n","\n","\n","# Show the result\n","df_moyenne_age.show()"],"metadata":{"id":"lqUVzcmutPOy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# nous voulons savoir les marques les plus utilisées par state:\n","df_marques_voitures = (df.select('auto_make', 'incident_state').groupBy('auto_make','incident_state').agg(count('auto_make').alias('auto_make_count')))\n","df_marques_voitures.show()"],"metadata":{"id":"t9uAwpeZtPE5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# nous voulons savoir les marques les plus utilisées:\n","df_auto_les_plus_utilisees = (df.select('auto_make').groupBy('auto_make').agg(count('auto_make').alias('auto_make_count')))\n","df_auto_les_plus_utilisees.show()"],"metadata":{"id":"_zr7IjSetO46"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"hpjm67uK3Qfo"},"execution_count":null,"outputs":[]}]}